---
title: "KR2"
author: "Isaev Stanislav"
date: '14 мая 2018 г '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Анализ главных компонент

Средние значения и дисперсия каждого регрессора.

```{r,echo=FALSE}
library ("MASS")
df <- Boston
df$chas <- as.numeric(df$chas)
df$rad <- as.numeric(df$rad)
apply(df, 2, mean)
apply(df, 2, var)
```

По последнему графику видно, что нам достаточно 4 компонент. Они суммарно объясняют более 70% дисперсии.

```{r,echo=FALSE}
pr.out=prcomp(df, scale=TRUE)
names(pr.out)
pr.out$center
biplot(pr.out, scale=0)
pr.out$rotation=-pr.out$rotation
pr.out$x=-pr.out$x
biplot(pr.out, scale=0)
pr.var=pr.out$sdev^2
pve=pr.var/sum(pr.var)
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,1),type='b')
plot(cumsum(pve), xlab="Principal Component", ylab="Cumulative Proportion of Variance Explained", ylim=c(0,1),type='b')
```

# Кластеризация

# Кластеризация по методу К средних

```{r,echo=FALSE}
x <- matrix(df$crim+df$zn+df$indus+df$chas+df$nox+df$rm+df$age+df$dis+df$rad+df$tax+df$ptratio+df$black+df$lstat+df$medv)
km.out=kmeans(x,2,nstart=1)
plot(x, col=(km.out$cluster+1), main="K-Means Clustering Results with K=2", xlab="", ylab="", pch=20, cex=2)
```

Если разбивать на три кластера, то результат можно считать приемлемым. 

```{r,echo=FALSE}
km.out=kmeans(x,3,nstart=1)
plot(x, col=(km.out$cluster+1), main="K-Means Clustering Results with K=3", xlab="", ylab="", pch=20, cex=2)
km.out=kmeans(x,3,nstart=1)
km.out$tot.withinss
km.out=kmeans(x,3,nstart=2)
km.out$tot.withinss
```

# Иерархическая кластеризация

Судя по графикам, используем метод "Complete"
```{r,echo=FALSE}
hc.complete=hclust(dist(x), method="complete")
hc.average=hclust(dist(x), method="average")
hc.single=hclust(dist(x), method="single")
par(mfrow=c(1,3))
plot(hc.complete,main="Complete Linkage", xlab="", sub="", cex=.9)
plot(hc.average, main="Average Linkage", xlab="", sub="", cex=.9)
plot(hc.single, main="Single Linkage", xlab="", sub="", cex=.9)
xsc=scale(x)
par(mfrow=c(1,1))
plot(hclust(dist(xsc), method="complete"), main="Hierarchical Clustering with Scaled Features", xlab="", sub="")
```

Суммарный отчет по регрессионной модели 1 кластера и средняя ошибка.
```{r,echo=FALSE}
y <- cutree(hc.complete,3)
df<-data.frame(df,y) 
kl1<-df[y==1,] 
kl2<-df[y==2,] 
kl3<-df[y==3,]

fit1 <- lm(crim ~ .-chas -ptratio -tax -lstat -indus -nox - dis -zn -age - medv, kl1)
summary(fit1)
train <- sample(1:nrow(kl1), nrow(kl1)/2)
test <- -train
z <- kl1$crim
z.test <- z[test]
round(mean((mean(z[train]) - z.test)^2), 0)
```

Суммарный отчет по регрессионной модели 2 кластера и средняя ошибка.
```{r,echo=FALSE}
fit2 <- lm(crim ~ .-medv -zn -dis -age -rad -rm -black -ptratio, kl2)
summary(fit2)
train <- sample(1:nrow(kl2), nrow(kl2)/2)
test <- -train
z <- kl2$crim
z.test <- z[test]
round(mean((mean(z[train]) - z.test)^2), 0)
```

Суммарный отчет по регрессионной модели 3 кластера и средняя ошибка.
```{r,echo=FALSE}
fit3 <- lm(crim ~ .- chas -rm -black -lstat, kl3)
summary(fit3)
train <- sample(1:nrow(kl3), nrow(kl3)/2)
test <- -train
z <- kl3$crim
z.test <- z[test]
round(mean((mean(z[train]) - z.test)^2), 0)
```



